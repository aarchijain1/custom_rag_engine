# RAG Agent with FastMCP

A production-ready Retrieval Augmented Generation (RAG) agent built with **FastMCP**, LangGraph, Google Gemini, and ChromaDB. Uses **True Model Context Protocol** for process isolation and scalability.

## ğŸ¯ Features

- **ğŸ”Œ FastMCP Servers**: Clean `@mcp.tool()` decorators for MCP protocol
- **ğŸ“¡ True MCP Protocol**: JSON-RPC 2.0 over stdio transport
- **ğŸ”„ Process Isolation**: MCP servers run as independent processes
- **ğŸ¤– Multi-Agent Architecture**: Master agent orchestrates retrieval and answering
- **ğŸ“š RAG System**: Semantic search with ChromaDB vector database
- **ğŸ’¬ Natural Conversation**: User-friendly chat interface
- **ğŸ” Smart Routing**: LLM decides when to use RAG vs direct answers
- **ğŸ“„ Multi-Format Support**: TXT, PDF, DOCX, JSON, Markdown
- **ğŸ’¾ Persistent Storage**: Local ChromaDB with 90 indexed chunks
- **âš¡ Offline Indexing**: Pre-build embeddings with `index_mcp.py`
- **ğŸ”§ Fully Configurable**: Easy customization via `config.py`
- **ğŸ› MCP Inspector**: Debug with official MCP tools
- **ğŸŒ No Cloud Required**: Runs locally (except Gemini API calls)

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   USER QUERY                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   RAG AGENT          â”‚
          â”‚  (LangGraph)         â”‚
          â”‚  - Master Router     â”‚
          â”‚  - Retrieval         â”‚
          â”‚  - RAG Answer        â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  MCP Client     â”‚ (Sync Wrappers)
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ stdio (JSON-RPC 2.0)
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                         â”‚
        â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FastMCP Server    â”‚    â”‚ FastMCP Server   â”‚
â”‚ Vector Store      â”‚    â”‚ Doc Loader       â”‚
â”‚                   â”‚    â”‚                  â”‚
â”‚ @mcp.tool()       â”‚    â”‚ @mcp.tool()      â”‚
â”‚ - search_docs()   â”‚    â”‚ - load_dir()     â”‚
â”‚ - add_docs()      â”‚    â”‚ - load_file()    â”‚
â”‚ - clear()         â”‚    â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ChromaDB      â”‚ (90 chunks from 6 documents)
â”‚   Embeddings    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
local-rag-agent/
â”œâ”€â”€ .env                          # API keys (create this)
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ README.md                     # This file
â”‚
â”œâ”€â”€ config.py                     # Configuration settings
â”œâ”€â”€ index_mcp.py                  # Offline indexing (via MCP)
â”œâ”€â”€ main_mcp.py                   # Runtime chat (via MCP)
â”‚
â”œâ”€â”€ mcp_server_vector.py          # FastMCP: Vector store server
â”œâ”€â”€ mcp_server_documents.py       # FastMCP: Document loader server
â”œâ”€â”€ mcp_client.py                 # MCP client with sync wrappers
â”‚
â”œâ”€â”€ agent_nodes_mcp.py            # LangGraph nodes (uses MCP)
â”œâ”€â”€ agent_graph.py                # LangGraph workflow
â”œâ”€â”€ vector_store.py               # ChromaDB implementation
â”œâ”€â”€ document_loader.py            # Multi-format document loader
â”œâ”€â”€ llm.py                        # Gemini LLM setup
â”‚
â”œâ”€â”€ documents/                    # Your documents (6 PDFs + 1 JSON)
â”‚   â”œâ”€â”€ TFS-lease-end-guide.pdf
â”‚   â”œâ”€â”€ faq_data.json
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ chroma_db/                    # Vector database (auto-created)
```

## ğŸš€ Quick Start

### 1. Prerequisites

- Python 3.9+
- Google Gemini API key (free from [Google AI Studio](https://aistudio.google.com/app/apikey))

### 2. Installation

```bash
# Clone or download this project
cd local-rag-agent

# Create virtual environment
python -m venv .venv

# Activate virtual environment
# On Windows:
.venv\Scripts\Activate.ps1
# On macOS/Linux:
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Install MCP SDK
pip install "mcp[cli]"
```

### 3. Configuration

Create a `.env` file in the project root:

```env
GOOGLE_API_KEY=your_gemini_api_key_here
```

### 4. Index Your Documents (Required First Step)

**Before you can chat, you must index your documents:**

```bash
# Documents are already in documents/ folder
# Run the indexing script via MCP
python index_mcp.py
```

Expected output:
```
ğŸ“š RAG INDEXER (MCP OVER STDIO)
ğŸ”Œ Connecting to MCP servers...
âœ“ Connected to MCP servers
âœ“ Loaded 6 documents
âš™ï¸ Indexing documents via MCP...
âœ… Indexing complete
Documents indexed : 6
Total chunks      : 90
```

This will:
- Connect to FastMCP servers via stdio
- Load documents via document loader MCP server
- Send documents to vector store MCP server
- Generate embeddings and store in ChromaDB

**Note:** Indexing is a one-time step (or run again when adding documents).

### 5. Run the Chat Application

```bash
python main_mcp.py
```

Expected output:
```
ğŸ¤– LOCAL RAG AGENT (MCP OVER STDIO)
âœ“ Connected to MCP servers
ğŸ“š Loaded index (90 chunks) via MCP
âœ“ RAG Agent ready
ğŸ’¬ CHAT STARTED

ğŸ§‘ User: 
```

## ğŸ“– Usage Guide

### Workflow: Index â†’ Chat

1. **Documents Provided**
2. **Index Documents**: Run `python index_mcp.py` 
3. **Chat**: Run `python main_mcp.py` to ask questions

### Indexing (index_mcp.py)

The indexing script uses **FastMCP** to process documents:

```bash
python index_mcp.py
```

**How it works:**
1. Connects to vector store MCP server
2. User asks question
3. Master router decides: RAG or direct answer?
4. If RAG: Calls `search_documents()` tool via MCP
5. Vector store server returns top 3 chunks
6. Gemini generates answer with context
7. Returns answer with sources

## ğŸ“ File Descriptions

| File | Purpose |
|------|---------|
| **MCP Servers** | |
| `mcp_server_vector.py` | FastMCP server for vector operations |
| `mcp_server_documents.py` | FastMCP server for document loading |
| `mcp_client.py` | MCP client with sync/async wrappers |
| **Application** | |
| `agent_nodes_mcp.py` | LangGraph nodes (uses MCP client) |
| `agent_graph_mcp.py` | LangGraph workflow builder |
| `index_mcp.py` | Offline indexing via MCP |
| `main_mcp.py` | Runtime chat interface via MCP |
| **Core Logic** | |
| `vector_store.py` | ChromaDB implementation |
| `document_loader.py` | Multi-format file loader |
| `llm.py` | Gemini LLM setup |
| `config.py` | All configuration settings |


## ğŸ‰ Success Checklist

Your system is working if:
- âœ… `python index_mcp.py` loads all 6 documents
- âœ… Shows "90 chunks" in stats
- âœ… `python main_mcp.py` starts without errors
- âœ… Agent answers questions about Toyota Financial Services
- âœ… Sources are included in responses
- âœ… Direct questions work without RAG
